{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZleKAebgoB42",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4401567a-882f-4bde-af4a-1ee5ae32e1a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tflearn\n",
            "  Downloading tflearn-0.5.0.tar.gz (107 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.3/107.3 KB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from tflearn) (1.21.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from tflearn) (1.15.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.8/dist-packages (from tflearn) (7.1.2)\n",
            "Building wheels for collected packages: tflearn\n",
            "  Building wheel for tflearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tflearn: filename=tflearn-0.5.0-py3-none-any.whl size=127299 sha256=f1eef94a8c8e4b142ab72473d4d599bfcf17bab970a8cfbedcec63b9de921c09\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/9b/15/cb1e6b279c14ed897530d15cfd7da8e3df8a947e593f5cfe59\n",
            "Successfully built tflearn\n",
            "Installing collected packages: tflearn\n",
            "Successfully installed tflearn-0.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tflearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFJKRLCDnvI5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42586190-45b4-46d6-a89d-95d3f0ea3d53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "import numpy as np\n",
        "import nltk, string\n",
        "import dill\n",
        "import pickle\n",
        "import torch\n",
        "import tflearn\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "\n",
        "nltk.download('punkt') # if necessary...\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = os.path.dirname(os.path.realpath('__file__'))"
      ],
      "metadata": {
        "id": "KscriVf6w86U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4jwrxTrc0qn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0072bdb0-fb3b-4885-c9ad-5c6c5b47a817"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "stemmer = nltk.stem.porter.PorterStemmer()\n",
        "remove_punctuation_map = dict((ord(char), None) for char in string.punctuation)\n",
        "\n",
        "list_words = []\n",
        "labels = []\n",
        "docs_x = [] \n",
        "docs_y = [] \n",
        "\n",
        "def cleanText(Text):\n",
        "    cleanedText = Text.replace('\\n',' ').lower()\n",
        "    cleanedText = cleanedText.replace(\"\\xc2\\xa0\", \"\")\n",
        "    cleanedText = re.sub('([,])','', cleanedText)\n",
        "    cleanedText = re.sub(' +',' ', cleanedText)\n",
        "    cleanedText = cleanedText.encode('ascii', 'ignore').decode('ascii')\n",
        "    return cleanedText\n",
        "\n",
        "def getData():\n",
        "    listTexts = source_data['abstract'].tolist()\n",
        "    finallist=[]\n",
        "    print(listTexts)\n",
        "    for i in range (0,4):\n",
        "        textDictionary = {\"tag\":i}\n",
        "        listTexts_i = cleanText(listTexts[i])\n",
        "        print(listTexts_i)\n",
        "        textDictionary.update(texts = listTexts_i)\n",
        "        finallist.append(textDictionary)\n",
        "    finalDictionary={\"intents\":finallist}       \n",
        "    return finalDictionary\n",
        "\n",
        "\n",
        "combinedDictionary = dict()\n",
        "print ('Getting Text Data')\n",
        "combinedDictionary.update(getData())\n",
        "print ('Total len of dictionary', len(combinedDictionary))\n",
        "\n",
        "print ('Saving text data dictionary')\n",
        "np.save(PATH + '/textDictionary.npy', combinedDictionary)\n",
        "\n",
        "with open(PATH + '/file.txt', 'w') as file:\n",
        "     file.write(json.dumps(combinedDictionary))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(PATH + \"/file.txt\") as file:\n",
        "    dataset = json.load(file)\n",
        "    \n",
        "for intent in dataset[\"intents\"]:\n",
        "    for pattern in intent[\"texts\"]: \n",
        "        split_words = nltk.word_tokenize(pattern) \n",
        "        list_words.extend(split_words) \n",
        "        docs_x.append(split_words) \n",
        "        docs_y.append(intent[\"tag\"]) \n",
        "    if intent[\"tag\"] not in labels:\n",
        "        labels.append(intent[\"tag\"]) \n",
        "\n",
        "list_words = [stemmer.stem(w.lower()) for w in list_words if w != \"?\"] \n",
        "list_words = sorted(list(set(list_words))) \n",
        "\n",
        "labels = sorted(labels)\n",
        "\n",
        "training = [] \n",
        "output = [] \n",
        "\n",
        "out_empty = [0 for _ in range(len(labels))]\n",
        "\n",
        "for x, doc in enumerate(docs_x):\n",
        "    bag = [] \n",
        "\n",
        "    split_words = [stemmer.stem(w.lower()) for w in doc] \n",
        "\n",
        "    for w in list_words:\n",
        "        if w in split_words:\n",
        "            bag.append(1) \n",
        "        else:\n",
        "            bag.append(0)\n",
        "\n",
        "    output_row = out_empty[:]\n",
        "    output_row[labels.index(docs_y[x])] = 1 \n",
        "\n",
        "    training.append(bag) \n",
        "    output.append(output_row) "
      ],
      "metadata": {
        "id": "KHLzLCO6znQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QF6FRDTJfv6t"
      },
      "outputs": [],
      "source": [
        "def bag_of_words(s, list_words): \n",
        "    bag = [0 for _ in range(len(list_words))]\n",
        "\n",
        "    inp_str_words = nltk.word_tokenize(s)\n",
        "    inp_str_words = [stemmer.stem(word.lower()) for word in inp_str_words]\n",
        "\n",
        "    for search_element in inp_str_words:\n",
        "        for i, w in enumerate(list_words):\n",
        "            if w == search_element:\n",
        "                bag[i] = 1 \n",
        "            \n",
        "    return np.array(bag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iO_ya6zIe-11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22149338-46d4-444c-edc8-406a8fe71919"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Step: 7437  | total loss: \u001b[1m\u001b[32m1.30984\u001b[0m\u001b[0m | time: 0.929s\n",
            "\u001b[2K\r| Adam | epoch: 014 | loss: 1.30984 - acc: 0.2777 -- iter: 1984/4423\n"
          ]
        }
      ],
      "source": [
        "training = np.array(training) \n",
        "output = np.array(output)\n",
        "\n",
        "net = tflearn.input_data(shape=[None, len(training[0])]) \n",
        "net = tflearn.fully_connected(net, 8) #Hidden layer with 8 neurons.\n",
        "net = tflearn.fully_connected(net, 8) #Another hidden layer with 8 neurons.\n",
        "net = tflearn.fully_connected(net, 8) #Another hidden layer with 8 neurons.\n",
        "net = tflearn.fully_connected(net, len(output[0]), activation=\"softmax\") \n",
        "\n",
        "model = tflearn.DNN(net) \n",
        "try:\n",
        "    model.predict(PATH + \"/model.tflearn\")\n",
        "    model.load()\n",
        "except:\n",
        "    model.fit(training, output, n_epoch=2000, batch_size=8, show_metric=True)\n",
        "    model.save(PATH + \"/model.tflearn\") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6N6DkhFglzIW"
      },
      "outputs": [],
      "source": [
        "def cosine_similarity(text1, text2):\n",
        "    tfidf = model.fit_transform([text1, text2])\n",
        "    return ((tfidf * tfidf.T).A)[0,1]\n",
        "\n",
        "    \n",
        "def is_plagiarism(similarity_score, plagiarism_threshold):\n",
        "\n",
        "  is_plagiarism = False\n",
        "\n",
        "  if(similarity_score >= plagiarism_threshold):\n",
        "    is_plagiarism = True\n",
        "\n",
        "  return is_plagiarism\n",
        "\n",
        "def run_plagiarism_analysis(query_text, data, plagiarism_threshold=0.8):\n",
        "\n",
        "    top_N=3\n",
        "\n",
        "    # Run similarity Search\n",
        "    data[\"similarity\"] = data[\"abstract\"].apply(lambda x: cosine_similarity(query_text, x))\n",
        "\n",
        "    similar_articles = data.sort_values(by='similarity', ascending=False)[0:top_N+1]\n",
        "    formated_result = similar_articles[[\"abstract\", \"similarity\"]].reset_index(drop = True)\n",
        "    similarity_score = formated_result.iloc[0][\"similarity\"] \n",
        "    similarity_percentage = str(round(similarity_score *100)) + '%'\n",
        "    most_similar_article = formated_result.iloc[0][\"abstract\"] \n",
        "    is_plagiarism_bool = is_plagiarism(similarity_score, plagiarism_threshold)\n",
        "\n",
        "\n",
        "    plagiarism_decision = {'similarity_score': similarity_score, \n",
        "                           'similarity_percentage': similarity_percentage,\n",
        "                            'is_plagiarism': is_plagiarism_bool,\n",
        "                            'most_similar_article': most_similar_article, \n",
        "                            'article_submitted': query_text\n",
        "                        }\n",
        "    #return plagiarism_decision\n",
        "    return formated_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bn6vp9LIl35p",
        "outputId": "c9d35d98-ccce-4a77-cb36-537259a4b603"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'similarity_score': 0.22576484600261604,\n",
              " 'similarity_percentage': '23%',\n",
              " 'is_plagiarism': False,\n",
              " 'most_similar_article': 'How safe are our microbiology labs?',\n",
              " 'article_submitted': 'hi, hello, how are you?'}"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "new_incoming_text = \"hi, hello, how are you?\"\n",
        "\n",
        "# Run the plagiarism detection\n",
        "analysis_result = run_plagiarism_analysis(new_incoming_text, source_data, plagiarism_threshold=0.8)\n",
        "analysis_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzLvpIuwmFM3",
        "outputId": "b73e44e9-b047-47b7-e845-1e51f8650bd4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'similarity_score': 1.0000000000000004,\n",
              " 'similarity_percentage': '100%',\n",
              " 'is_plagiarism': True,\n",
              " 'most_similar_article': 'A critical aspect of reliable communication involves the design of codes that\\nallow transmissions to be robustly and computationally efficiently decoded\\nunder noisy conditions. Advances in the design of reliable codes have been\\ndriven by coding theory and have been sporadic. Recently, it is shown that\\nchannel codes that are comparable to modern codes can be learned solely via\\ndeep learning. In particular, Turbo Autoencoder (TURBOAE), introduced by Jiang\\net al., is shown to achieve the reliability of Turbo codes for Additive White\\nGaussian Noise channels. In this paper, we focus on applying the idea of\\nTURBOAE to various practical channels, such as fading channels and chirp noise\\nchannels. We introduce TURBOAE-TI, a novel neural architecture that combines\\nTURBOAE with a trainable interleaver design. We develop a carefully-designed\\ntraining procedure and a novel interleaver penalty function that are crucial in\\nlearning the interleaver and TURBOAE jointly. We demonstrate that TURBOAE-TI\\noutperforms TURBOAE and LTE Turbo codes for several channels of interest. We\\nalso provide interpretation analysis to better understand TURBOAE-TI.',\n",
              " 'article_submitted': 'A critical aspect of reliable communication involves the design of codes that\\nallow transmissions to be robustly and computationally efficiently decoded\\nunder noisy conditions. Advances in the design of reliable codes have been\\ndriven by coding theory and have been sporadic. Recently, it is shown that\\nchannel codes that are comparable to modern codes can be learned solely via\\ndeep learning. In particular, Turbo Autoencoder (TURBOAE), introduced by Jiang\\net al., is shown to achieve the reliability of Turbo codes for Additive White\\nGaussian Noise channels. In this paper, we focus on applying the idea of\\nTURBOAE to various practical channels, such as fading channels and chirp noise\\nchannels. We introduce TURBOAE-TI, a novel neural architecture that combines\\nTURBOAE with a trainable interleaver design. We develop a carefully-designed\\ntraining procedure and a novel interleaver penalty function that are crucial in\\nlearning the interleaver and TURBOAE jointly. We demonstrate that TURBOAE-TI\\noutperforms TURBOAE and LTE Turbo codes for several channels of interest. We\\nalso provide interpretation analysis to better understand TURBOAE-TI.'}"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "new_incoming_text =  source_data.iloc[1]['abstract']\n",
        "\n",
        "# Run the plagiarism detection\n",
        "analysis_result = run_plagiarism_analysis(new_incoming_text, source_data, plagiarism_threshold=0.8)\n",
        "analysis_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zY70b1Im2hg",
        "outputId": "09d8a8f6-9bd6-4b23-be46-a3c719a2d2ab"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'similarity_score': 0.18381708471385255,\n",
              " 'similarity_percentage': '18%',\n",
              " 'is_plagiarism': False,\n",
              " 'most_similar_article': 'BACKGROUND Nucleic acid test and antibody assay have been employed in the diagnosis for SARS-CoV-2 infection, but the use of viral antigen for diagnosis has not been successfully developed. Theoretically, viral antigen is the specific marker of the virus and precedes antibody appearance within the infected population. There is a clear need of detection of viral antigen for rapid and early diagnosis. METHODS We included a cohort of 239 participants with suspected SARS-CoV-2 infection from 7 centers for the study. We measured nucleocapsid protein in nasopharyngeal swab samples in parallel with the nucleic acid test. Nucleic acid test was taken as the reference standard, and statistical evaluation was taken in blind. We detected nucleocapsid protein in 20 urine samples in another center, employing nasopharyngeal swab nucleic acid test as reference standard. RESULTS We developed a fluorescence immunochromatographic assay for detecting nucleocapsid protein of SARS-CoV-2 in nasopharyngeal swab sample and urine within 10 minutes. 100% of nucleocapsid protein positive and negative participants accord with nucleic acid test for same samples. Further, earliest participant after 3 days of fever can be identified by the method. In an additional preliminary study, we detected nucleocapsid protein in urine in 73.6% of diagnosed COVID-19 patients. CONCLUSIONS Those findings indicate that nucleocapsid protein assay is an accurate, rapid, early and simple method for diagnosis of COVID-19. Appearance of nucleocapsid protein in urine coincides our finding of the SARS-CoV-2 invading kidney and might be of diagnostic value.',\n",
              " 'article_submitted': 'possible further increases in nucleic acid'}"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "pickle.dump(run_plagiarism_analysis, open('similarity_model.pkl','wb'))\n",
        "g = pickle.load(open('similarity_model.pkl','rb'))\n",
        "g(\"possible further increases in nucleic acid\", source_data, plagiarism_threshold=0.8)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}